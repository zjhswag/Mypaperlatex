Automatic Text Summarization (ATS) utilizes computational models to perform automated semantic compression and reconstruction of lengthy source documents. Its goal is to generate concise, coherent summaries that retain core semantics, thereby addressing the issue of information overload while ensuring the factuality, fluency, and logical consistency of the generated content. Although the advent of the Large Language Model (LLM) era has driven significant advancements in ATS, current models still suffer from hallucinations and context window limitations. Therefore, it is crucial to ensure that generation remains grounded in the factual content of source documents. Consequently, this thesis integrates the respective strengths of extractive models and LLMs, proposing a hybrid approach for multi-turn summary generation. The main work and contributions are summarized as follows:

(1) Addressing the issue of key information omission caused by semantic dilution and positional bias in documents, this paper introduces a dual-stream semantic interaction mechanism based on the BertSum extractive model. First, a topic model is utilized to mine explicit document topics. Through a topic-biased attention mechanism, global topic information is injected into the self-attention layer as an additive bias, reshaping the attention distribution to enhance the capture of long-distance key features. Second, a global gating fusion module is designed to dynamically calibrate sentence representations using document-level topic vectors, thereby mitigating the inherent positional bias of pre-trained models. Furthermore, the feature space is optimized through a hybrid training objective, driving the model to regularize feature distribution via topic anchors in the latent semantic space. Finally, experiments on multiple standard datasets demonstrate the significant performance advantages of the proposed method.

(2) Addressing the hallucination risks and logical incoherence prevalent in generative models, this paper proposes an Evidence Chain-driven "Generate-Evaluate-Refine" closed-loop framework. First, extracted key sentences are used to construct a high signal-to-noise ratio evidence chain, with semantic fragmentation repaired via a local context window enhancement algorithm. Subsequently, a Chain-of-Thought (CoT) based iterative optimization mechanism is introduced: the generator produces a preliminary draft based on the evidence chain, while the evaluator—utilizing structured prompt engineering—inspects the summary for factual consistency, semantic coherence, and textual logic. This process triggers predefined correction instructions such as addition, deletion, and rewriting. Through multi-turn iterations, the final summary is progressively generated. Additionally, to address potential information loss in static evidence chains, the framework integrates feedback-driven dynamic retrieval and semantic compression algorithms. When the evaluator detects information gaps, it automatically initiates secondary retrieval and compresses the textual information.

(3) To translate the aforementioned theoretical research into a practical tool, this paper designs and develops a cloud-based intelligent document summarization and editing system. The system supports high-fidelity conversion of formats such as DOCX and PDF, allowing users to upload local documents for convenient online editing and storage. Furthermore, the system integrates the proposed algorithms via the Model Context Protocol (MCP), supporting deep Q\&A and summary generation based on document content. This enables users to efficiently grasp key research content, query complex sections to acquire new knowledge, and draft documents with the assistance of the model.